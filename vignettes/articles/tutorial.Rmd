---
title: "[GER] Tutorial"
author: "Thomas von Allmen"
date: "`r Sys.Date()`"
---

```{r setup, include = FALSE}
set.seed(1014)
options(digits = 3)

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(dplyr)
library(magrittr)
```

# Einleitung

Dieses Dokument soll einem Anwender des Packages `rOstluft::rOstluft` einen Überblick der enthaltenen Funktionalität 
bieten und die gängigsten Arbeitsabläufe aufzeigen.

Der Fokus dieses Packages liegt in der Bereitstellung von Daten aus verschiedenen Datenquellen in einem einheitlichen 
Format zur Analyse der Luftqualität. Ausserdem enthält es Werkzeuge für einige übliche Aufgaben, die während solchen 
Analysen anfallen. Diese sind Umrechnungen zwischen verschiedenen Mittelungsintervallen, Statistische Methoden mit 
Berücksichtung der Datenverfügbarkeit, Umwandlung von Volumen- und Massenkonzentrationen und lesen von Daten vorliegend 
in verschiedenen Formaten.

# Installation

Der Quellcode von [rOstluft](https://github.com/Ostluft/rOstluft) ist auf github gehosted. Die einfachste Variante ist 
die Installation mit Hilfe des Packages devtools:

```{r eval=FALSE}
#install.packages("devtools")
devtools::install_github("Ostluft/rOstluft")
```

Ist dies wegen Einschränkungen durch Firewalls oder Proxies nicht möglich. Muss der Quellcode manuell von github 
heruntergeladen werden (Clone or download > Download as ZIP), entpackt und manuell installiert werden. Allerdings 
bestehen Abhängigkeiten zu Packages die auf CRAN bereitgestellt werden. Können auch keine CRAN Packages installiert 
werden, müssen zuerst alle CRAN Abhängkigkeiten und deren Abhängigkeiten installiert werden. 

Zusätzlich besteht noch die Github Abhängkigkeit zu [rOstluft.data](https://github.com/Ostluft/rOstluft.data). Dieses 
Packages muss auf die gleiche Weise zuerst installiert werden mit folgenden Schritten:

* Download der Quellen: [rOstluft.data](https://github.com/Ostluft/rOstluft.data) und 
  [rOstluft](https://github.com/Ostluft/rOstluft)
* In RStudio Tools > Install Packages
* In der Auswahlbox "Install from" das Feld "Package Archive File (.zip; .tar.gz)" auswählen
* Im Datei Dialog die Datei von rOstluft.data auswählen
* Mit der Schaltfläche "Install" das Package installieren
* Wiederholen für rostluft

Nach der Installation kann das Packages verwendet werden:

```{r}
library(rOstluft)
```


# Abfrage von Daten

## Ostluft Amazon AWS S3

Die zentrale Datenablage innerhalb von Ostluft erfolgt auf [Amazon AWS S3](https://aws.amazon.com/s3/).Aus 
Lizenztechnischen Gründen kann das Bucket nicht öffentlich zugänglich gemacht werden. Die Zugangsdaten werden von Jörg 
Sintermann vergeben. Die Zugangsdaten werden am einfachsten über eine .Renvirion Datei im Verzeichnis des RStudio 
Projekts dem Package zugänglich gemacht. Inhalt der .Renvirion Datei:

```
AWS_ACCESS_KEY_ID = "XXXXXXXXXXXXXXXXXXXX"
AWS_SECRET_ACCESS_KEY = "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
AWS_DEFAULT_REGION = "eu-central-1"
```

Weitere Möglichkeiten sind in der Dokumentation von [aws.signature](https://github.com/cloudyr/aws.signature/) zu 
finden.

Sämtliche Daten die einmal von Amazon S3 geöffnet wurden, werden lokal auf dem Rechner gespeichert. Bei jedem folgenden 
Zugriff wird nur überprüft, ob die Daten noch identisch sind.

Der Zugriff auf die Daten erfügt dann über ein store Objekt, welches auf folgende Art initialisiert wird:

```{r}
store <- storage_s3_rds("tutorial", format = format_rolf(), bucket = "rostluft", prefix = "aqmet")
```

*TODO: Beschreibung was die verschiedenen Argument bedeuten*


Dieses store Objekt verfügt über verschiedene Methoden, zur Abfrage von Daten sind jedoch nur zwei von Bedeutung:

* `$get_content()`: holt eine Übersicht über alle im store enthaltene Daten
* `$get()`: Aktuelle Datenabfrage. Welche Daten geholt werden, muss über Funktionsargumente definiert werden

```{r}
content <- store$get_content()
content

dplyr::sample_n(content, 10)
```

Eine Datenabfrage holt immer die Daten von einem komplettem Jahr einer Station für einen bestimmten Mittelungszeitraum:

```{r}
store$get(site = "Zch_Schimmelstrasse", year = 2014, interval = "min30")
```

Jedes der Argumente kann auch ein Vector sein mit mehreren Werte. Es wird dann Kombination aller Möglichkeiten 
abgefragt.

```{r}
sites <- c("Zch_Schimmelstrasse", "Zch_Rosengartenstrasse", "Zch_Stampfenbachstrasse")
data <- store$get(site = sites, year = 2014:2015, interval = "min30")
data

dplyr::sample_n(data, 10)
```

Die letzte Funktionalität von `$get()` ist die Filterung der Daten vor der Rückgabe. Mit dem Argument "filter" als 
`dplyr::filter()` kompatibler Ausdruck:

```{r}
store$get(site = sites, year = 2014, interval = "min30", filter = parameter == "PM10")
```


## Lokales Arbeiten mit den AWS S3 Daten

Ein Nachteil des S3 Storage ist, dass bei jeder Datenabfrage eine Internetverbindung zur Überprüfung ob aktualisierte 
Daten verfügbar sind vorhanden sein muss. Selbst wenn die Daten bereits lokal gespeichert sind. Nicht nur wird eine 
Internetverbindung benötigt, die Überprüfung braucht auch eine kurze Zeit. Hat man alle notwendigen Daten bereits 
herunter geladen, kann man jedoch mit einem lokalen Store arbeiten. Praktischerweise hat der S3 Store eine Funktion, 
welche einen lokalen Store zurückgibt.

```{r}
lokal <- store$get_local_storage()
```

Dieser verfügt über die gleichen Funktionalität wie der S3 Store:

```{r}
lokal$get_content()

lokal$get(site = sites, year = 2014, interval = "min30", filter = parameter == "PM10")
```

Dem aufmerksamen Leser ist vermutlich aufgefallen, dass die Inhaltsübersicht von `$get_content()` nicht dem lokalen 
Inhalt entspricht. Es ist immer noch die Übersicht welche Daten in S3 verfügbar wären. Will man wissen welche Daten 
lokal verfügbar sind, hilft einem die Funktion `$list_chunks()` weiter. 

```{r}
tibble::glimpse(lokal$list_chunks())
```

Auch der S3 Store verfügt über die `$list_chunks()` Funktion.

```{r}
tibble::glimpse(store$list_chunks())
```

Im normal Fall ist Nutzung von `$get_content()` über `$list_chunks()` zu bevorzugen. Die Rückgabe von `$get_content()` 
enthält die gemessen Parameter und die Anzahl der gültigen Punkte. Bei `$list_chunks()` hingegen sind einige interne 
Informationen über den Store enthalten.

Um die Vorbereitung um mit einem lokalen Store zu arbeiten zu vereinfachen verfügt der S3 Store über die Funktion 
`$download(...)`. Die dot Argumente werden als Filter auf die Rückgabe von `$list_chunks()` angewendet. Ohne Argumente 
wird der komplette Store heruntergeladen. Folgendes Beispiel lädt samtliche 30 Minutenmittelwerte für die Station 
Rosengartenstrasse nach dem Jahr 2015 (Achtung == verwenden, es sind Filter Ausdrücke!):

```{r}
store$download(site == "Zch_Rosengartenstrasse", interval == "min30", year > 2015)

lokal$list_chunks() %>% dplyr::select("site", "year", "interval")
```


## Eigene Daten in einem lokalen Store

Der aqmet S3 Store enthält nur bereinigte Daten von abgeschlossenen Jahren. Werden Daten benötigt, die nicht in S3 
vorhanden sind, bietet sich die Nutzung eines seperaten lokalen Stores an:

```{r}
my_store = storage_local_rds("eigene_daten", format_rolf(), read.only = FALSE)
```

In diesen kann man nun Daten mit der Store Funktion `$put()` oder der Hilfsfunktion `import_directory()` 
importieren:

```{r}
examples_path <- system.file("extdata", package = "rOstluft.data")
import_directory(my_store, examples_path, read_airmo_csv, glob = "*Jan.csv")

fn <- fs::path(examples_path, "Zch_Rosengartenstrasse_2010-2014.csv")
my_data <- read_airmo_csv(fn)
my_store$put(my_data)
```

Wie gewohnt kann erhält man mit `$get_content()` eine Übersicht und die Daten mit `$get()`:

```{r}
my_store$get_content()

my_store$get(site = "Zch_Stampfenbachstrasse", interval = c("d1", "h1"), year = 2013)  %>% 
  dplyr::arrange(.data$starttime, .data$parameter)
```


# Einlesefunktionen von Daten

Um Daten in R einzulesen exisitieren für folgende Quellen bereits Funktionen:

* Ostluft: `read_airmo_csv()` und `read_airmo_dat()` für Exporte aus der AIRMO
* MeteoSchweiz: `read_smn()` und `read_smn_multiple()` für SwissMetNet Exporte
* ETHZ/IAC: `read_ethz_iac()` Meteo Stationen auf dem Hönggerberg und dem Hauptgebäude CHN

Existiert für die vorliegenden Daten keine Funktion und man möchte die selber schreiben an dieser Stelle 
einige Hinweise und Tipps.


## Verwende readr Funktionen und definiere die Zeichenkodierung

Ein generelles Problem beim einlesen von Textdateien ist die [Zeichenkodierung](https://de.wikipedia.org/wiki/Zeichenkodierung). 
Die meisten haben vermutlich "\uFFFD" schon in Texten gesehen. Alle Daten im aqmet Store und alle Rückgabewerte von 
`read_xxx()` Funktionen sind UTF-8 kodiert. Mit dem [readr Packages](https://readr.tidyverse.org/index.html) wird bei 
korrekter Definition der [Locale](https://readr.tidyverse.org/articles/locales.html#character) der Text automatisch 
zu UTF-8 konvertiert. Im Gegensatz zu den R Base Funktionen oder data.table::fread().


## Ausgabeformat rolf

Für ein reibungsloses Zusammenspiel mit den Stores zu garantieren muss das Format rolf eingehalten werden. rolf setzt 
einen tibble mit folgenden Spalten und Klassen voraus:

* **"starttime"**: POSIXct
* **"site"**: factor
* **"parameter"**: factor
* **"interval"**: factor
* **"unit"**: factor
* **"value"**: double

Selbst wenn in der Datei keine Einheiten enthalten sind, ist es besser die Spalte unit mit NA zu initialisieren. Ein 
weiterer Trick ist es die Spalten am Ende explizit zu selektionieren. Hier ein Beispiel, wie es in `read_smn()` gelöst 
ist:

```{r eval=FALSE}
data <- dplyr::mutate(data,
  stn = forcats::as_factor(.data$stn),
  parameter = forcats::as_factor(.data$parameter),
  interval = forcats::as_factor(interval),
  unit = factor(NA)
)
dplyr::select(data, starttime = "time", site = "stn", "parameter", "interval", "unit", "value")
```

Mit dem `dplyr::select` am Ende ist man sicher nur die Spalten in der Rückgabe zu haben die man haben will und es ist 
einfach noch Spalten umzubenennen und die Reihenfolge zu ändern.


# Zusammenfügen von Daten im rolf Format

Die Verwendung von Factoren im rolf Format hat seine Vor- und Nachteile. Der Hauptvorteil liegt bei schnelleren Lese- 
und Schreiboperation, der grösste Nachteil beim Zusammenfügen der Daten. `dplyr::bind_rows` wird die Factoren in 
Characters umwandeln, wenn unterschiedliche Levels in den Daten vorhanden sind. 
Die Funktion `bind_rows_with_factor_columns()` führt das Zusammenfügen korrekt aus. Sie unterstützt ausserdem 
[Quasiquotation](https://rlang.r-lib.org/reference/quasiquotation.html) von rlang[^1]. Somit kann auch eine grosse Liste 
von tibbles im rolf Format komfortable aneinander gefügt werden:

```{r}
df1 <- read_airmo_csv(fs::path(examples_path, "Zch_Stampfenbachstrasse_h1_2013_Jan.csv"))
df2 <- read_airmo_csv(fs::path(examples_path, "Zch_Stampfenbachstrasse_d1_2013_Jan.csv"))
df_comb <- bind_rows_with_factor_columns(df1, df2)
df_comb %>% dplyr::arrange(.data$starttime, .data$parameter)

# liste mit 20+ tibbles
df_list <- read_smn_multiple(fs::path(examples_path, "smn_multi.txt"), as_list = TRUE)
length(df_list)
df_comb <- bind_rows_with_factor_columns(!!!df_list)
df_comb %>% dplyr::arrange(.data$starttime, .data$parameter)
```

`bind_rows_with_factor_columns()` entfernt jedoch keine Duplikate aus den Daten. Es hängt die Daten einfach 
aneinander. Möchte man Duplikate entfernen, bzw. Daten aktualisieren mit neueren Daten muss man die Funktion `$merge()` 
des Format Objekts benützen. Dieses wurde beim Erzeugen des Store direkt initialisiert und ist als Feld `$format` 
verfügbar. Alternativ kann ein seperates Format Objekt erzeugt werden. Die Daten im ersten Datenframe werden über Daten 
im zweiten Datenframe priorisiert.

```{r}
# kopiere Daten und setze einige NA -> Resultat sollte NA enthalten
df1 <- df2
df1$value[1:150] <- NA  

df_comb <- store$format$merge(df1, df2)
df_comb %>% dplyr::arrange(.data$starttime, .data$parameter)

rolf <- format_rolf()
df_comb <- rolf$merge(df1, df2)
df_comb %>% dplyr::arrange(.data$starttime, .data$parameter)
```

Die `$merge()` Funktion unterstützt nur 2 Argumente. Muss eine Liste von tibbles gemerget werden muss man 
`purrr::reduce()` benützen. Mit Hilfe des Argument `.dir` kann man die Priosierung setzen. Mit "forward"" haben die 
Daten die zuerst in der Liste auftauchen Priorität, mit "backward" die Letzten.

```{r}
df_comb <- purrr::reduce(df_list, rolf$merge, .dir = "forward")
df_comb %>% dplyr::arrange(.data$starttime, .data$parameter)
```


# Metadaten

Daten aus verschiedenen Quellen werden verschiedene Schemen für die Vergabe von Namen haben. MeteoSchweiz zum Beispiel 
kodiert im Parameter Namen gleichzeitig das Mittelungsinterval:

| Parameter | Interval                  | Messgrösse                               |
|-----------|---------------------------|------------------------------------------|
| ta1towb0  | Bürgerliche Stundenmittel | Lufttemperatur, Instrument 1             |
| ta1towh0  | Stundenmittel             | Lufttemperatur, Instrument 1             |
| ta1tows0  | Momentanwerte             | Lufttemperatur, Instrument 1             |
| fk1towz0  | Zehnminutenmittel         | Windgeschwindigkeit skalar, Instrument 1 |


Um die Bezeichnungen zu normalisieren kommt es zu einem Zusammenspiel von im Store bereitgestellte Daten und der 
Funktion `meta_apply()`. Das Store Objekt verfügt über die Funktion `$get_meta()`, welche im AWS S3 bereitgestellte 
Metadaten zurück gibt. Wird `$get_meta()` ohne Argument aufgerufen erhält man eine Liste mit sämtlichen Metadaten im 
Store. Kennt man bereits den Namen kann man direkt die entsprechenden Metadaten holen: 


```{r}
meta <- store$get_meta()
names(meta)

meteoschweiz <- store$get_meta("meteoschweiz")
tibble::glimpse(meteoschweiz)
```


Die `meta_apply()` benützt die Metadaten als "Lookup Table". Es wird folgende Operation ausgeführt:

```{r eval=FALSE}
meta_apply(data, meta, data_src, data_dest, meta_key, meta_val) 

data$data_dest = meta[meta$meta_key == data$data_src]$meta_val
```

Wobei die jeweiligen Spalten jeweils als Argument übergeben werden. Die Funktion verfügt über verschiende Modi um mit 
fehlenden Einträgen umzugehen. Im folgenden Beispiel werden bei den MeteoSchweiz Daten die "unit" Spalte basierend 
auf den "parameter" Spalte ausgefüllt. Da die Parameter noch die MeteoSchweiz Bezeichnungen haben müssen wir als Meta 
Key die Spalte "parameter_original" und "unit" als Value verwenden. Es werden die Daten aus rOstluft.data verwendet. In 
diesen fehlt in den Metadaten der MeteoSchweiz Parameter rre150z0 (Niederschlag). Der Default Modus "strict" stoppt 
das Script:

```{r}
meteoschweiz <- readRDS(fs::path(examples_path, "meta_smn.rds"))
data <- read_smn(fs::path(examples_path, "smn.txt"), na.rm = FALSE)
data <- dplyr::arrange(data, .data$starttime, .data$parameter) 
data

tryCatch({
  meta_apply(data, meteoschweiz, "parameter", "unit", "parameter_original", "unit")
}, error = function(e) {
  sprintf(e$message)
})
```



Im einfachsten Fall werden Zeilen mit fehlenden Einträge einfach gelöscht. Dies geschieht mit dem Modus "drop":

```{r}

df <- meta_apply(data, meteoschweiz, "parameter", "unit", "parameter_original", "unit", mode = "drop")
df
```

Sollte der Parameter für spätere Bearbeitung behalten werden, kommt der Modus "keep" zum Zug:

```{r}
df <- meta_apply(data, meteoschweiz, "parameter", "unit", "parameter_original", "unit", mode = "keep")
df
```

Mit Hilfe des Modus "replace" kann eine zusätzliche "Lookup Table" übergeben werden. Werte in dieser Tabelle haben 
Priorität über Werte in der Meta Tabelle. Im Beispiel wird die fehlende Einheit für rre150z0 bereit gestellt und die 
Einheit für dkl010z0 von ° zu deg überschrieben. 

```{r}
df <- meta_apply(data, meteoschweiz, "parameter", "unit", "parameter_original", "unit", 
                 mode = "replace", replacements = list(rre150z0 = "mm", dkl010z0 = "deg"))
df
```

Hier ein Beispiel für die komplette Überführung der MeteoSchweiz Daten, inkl. manuelle Umbennung von rre150z0 und 
Änderung der Einheit für die Windrichtung (WD) von ° zu deg:

```{r}
df <- meta_apply(data, meteoschweiz, "parameter", "unit", "parameter_original", "unit", 
                 mode = "replace", replacements = list(rre150z0 = "mm", dkl010z0 = "deg"))

df <- meta_apply(df, meteoschweiz, "parameter", "parameter", "parameter_original", "parameter", 
                 mode = "replace", replacements = list(rre150z0 = "Niederschlag"))

df <- meta_apply(df, meteoschweiz, "site", "site", "site_short", "site")

df
```

# openair kompatibles Wide Format

Das Long Format ist nicht für jede Analyse geeignet. Aus diesem Grund enthält rOstluft die Funktionalität Daten im rolf 
Format in ein [openair](http://davidcarslaw.github.io/openair/) kompatibles Wide Format umzuwandeln. Ein Nachteil am 
Wide Format ist, dass Informationen die zu einer Spalte gehören, wie die Einheit einer Messgrösse, verloren gehen. Dies 
führt auch dazu, das man nicht die gleiche Messgrösse mit unterschiedlichen Einheiten in den Daten haben kann. Ausser 
man nennt die Messgrösse um. Die Funktion `rolf_to_openair()` geht folgende Kompromisse ein: Die Einheiten werden 
entfernt, aber im Attribute "units" des Datenframes gespeichert. Messgrössen mit der Einheit ppb oder ppm werden 
ebenfalls entfernt (Dieser Automatismus lässt sich mit dem Argument keep_ppb abschalten). Openair nimmt an, dass alle 
Messgrössen als Massenkonzentrationen vorliegen. Weitere Konventionen in openair sind das die Zeitinformation in der 
Spalte "date" als `Date` oder `POSIXct` vorliegen, die Windgeschwindigkeit in der Spalte "ws" und die Windrichtung in 
der Spalte "wd". Sämtliche Character und Factor Spalten dienen als Gruppierungsspalten. Diese werden von openair 
ignoriert oder gar entfernt, wenn nicht das Argument type benützt wird. Dieses erlaubt die explizite Nutzung einer 
Spalte als Gruppierungskriterium. Es können mehrere Gruppierungskriterien getrennt durch ein Komma definiert werden. 
Die openair Funktion `openair::cutData()` übernimmt die Gruppierung.

```{r}
data <- read_airmo_csv(fs::path(examples_path, "Zch_Stampfenbachstrasse_2010-2014.csv"))
wide <- rolf_to_openair(data)
tibble::glimpse(wide)
```

Diese Daten können nun direkt in openair verwendet werden:

```{r}
openair::windRose(wide)

openair::polarPlot(wide, "NO2", type="year")
```

`openair_to_rolf()` konventiert Daten zurück ins rolf Format. Der Nutzer muss die fehlenden Informationen bereitstellen:

```{r}
openair_to_rolf(wide, interval = "min30", units = attr(wide, "units"))
```

`rolf_to_openair_single()` bietet die Möglichkeit einen bestimmten Parameter rauszupicken:

```{r}
rolf_to_openair_single(data, "NO2", unit = "µg/m3", keep_interval = TRUE)
```

Die weitere Optionen für die Funktion `rolf_to_openair()` sind in der Dokumentation zu finden.


# Resampling von Daten


# Einheitenumwandlung


# Lokale Daten löschen

Werden die Daten lokal nicht mehr benötigt, kann mit der Funktion `$destroy()` des Store Objektes die Daten gelöscht 
werden. Zu beachten ist, dass der AWS S3 Store als read only initialisiert wurde. Um ihn löschen zu können muss dies 
geändert werden. Um ein versehentliches löschen durch Autocomplete zu verhindern, muss der String "DELETE" als Argument 
übergeben werden

```{r}
store$read.only = FALSE
store$destroy("DELETE")

my_store$destroy("DELETE")
```



[^1]: rlang muss nicht als library importiert werden. ... leitet die Expression an rOstluft weiter. Dieses hat den 
  Operator importiert und kann somit die Expression korrekt auswerten.
